{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generartive Adversarial Network\n",
    "In this notebook, I will be working on a generative adversarial network. There are two main components in a network like this:\n",
    "1. Generator : A generative neural network that attempts to generate synthetic fake data from the training set.\n",
    "2. Discriminator : Another network that attempts to tell the difference between fake and real data from the generator and training set.\n",
    "\n",
    "The way this network learns is like a back and forth game, where they keep trying to trick each other.\n",
    "\n",
    "Here are some of the resources used for this study:\n",
    "1. https://www.youtube.com/watch?v=_pIMdDWK5sc\n",
    "2. https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/basic-gan.html\n",
    "3. https://developers.google.com/machine-learning/gan/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing all the necessry modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting constants\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the MNIST Dataset Loader module, this has been taken from https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/basic-gan.html\n",
    "\n",
    "(Purely because I didn't want to spend too much time writing this, since it's readily available lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = PATH_DATASETS,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        num_workers: int = NUM_WORKERS,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dims = (1, 28, 28)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "Now, let's build the Generator. This is going to be a Convolutional Neural Network (CNN) that attempts to generate fake images based on the MNIST set. \n",
    "\n",
    "The general structure of this network is going to be \n",
    "\n",
    "Input - > Linear Layer - > Upsample Layer 1 - > Upsample Layer 2 - > Convolution to 28 x 28\n",
    "\n",
    "*A note on the dimensions of the convolution layers:*\n",
    "\n",
    "At the beginning of the generation process, the feature maps need a high number of channels (e.g., 64) to capture diverse patterns and encode the necessary complexity of the image. At this stage, the spatial dimensions are small (7×7), so there's room for more depth in feature representation.\n",
    "\n",
    "\n",
    "As the spatial resolution increases (14×14), then (28×28), the feature maps focus more on refining the image's structure and texture rather than adding new abstract details. Thus, fewer channels are needed.\n",
    "\n",
    "The dimensions of the outputs from the transposed convolution (upsamping) layers can be calculated using the following formula:\n",
    "\n",
    "$$\n",
    "H_{\\text{out}} = (H_{\\text{in}} - 1) \\cdot \\text{stride} - 2 \\cdot \\text{padding} + \\text{kernel\\ size}\n",
    "$$\n",
    "* *H here is Height, which is equal to width in our case* \n",
    "\n",
    "The same for the convolution layer can be calculated using:\n",
    "\n",
    "$$\n",
    "H_{\\text{out}} = \\frac{H_{\\text{in}} + 2 \\cdot \\text{padding} - \\text{kernel\\ size}}{\\text{stride}} + 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(latent_dim, 7*7*64) \n",
    "        self.convTrans1 = nn.ConvTranspose2d(64, 32, 4, stride=2)\n",
    "        self.convTrans2 = nn.ConvTranspose2d(32, 16, 4, stride=2)\n",
    "        self.conv = nn.Conv2d(16, 1, kernel_size=7)\n",
    "\n",
    "    def foward(self, x):\n",
    "        # Reshaping the latent space\n",
    "        x = self.linear(x)\n",
    "        x = func.relu(x)\n",
    "        x = x.view(-1, 64, 7, 7)\n",
    "        \n",
    "        # Upsampling to 16x16\n",
    "        x = self.convTrans1(x)\n",
    "        x = func.relu(x)\n",
    "        \n",
    "        # Upsampling to 34x34\n",
    "        x = self.convTrans2(x)\n",
    "        x = func.relu(x)\n",
    "\n",
    "        # Convoluting back to 28x28\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "Now that we have our generator, it's time to build our Discriminator. If the generator is the criminal, this guy is our detective to tell apart from the real and fake images.\n",
    "\n",
    "When we get to programming the loss functions for both of these networks, the discriminator actually plays a very important role in backpropogation for both networks (Making the generator make better fake images, and teach the discriminator to judge better). But, we will get to that at the training stage.\n",
    "\n",
    "This is also a convolutional neural network, a more traditional 2-layer one to classify whether the image is fake or not (will output 0 or 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = func.relu(func.max_pool2d(self.conv1(x), 2))\n",
    "        x = func.relu(func.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = func.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
