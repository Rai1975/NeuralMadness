{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Neural Network\n",
    "\n",
    "In this notebood, I will be attempting to create my own transformer neural network from scratch. As many of you know, this is literally how chatGPT and many famous LLMs work under the hood. I'm excited to try this out, to further understand how this stuff ACTUALLY works\n",
    "\n",
    "<img src=\"assets/Screenshot 2024-11-12 144358.png\">\n",
    "\n",
    "Note: I will be using pytorch for this one, but I will later try to implement one from scratch similar to my previous project. But we shall see :salute_face:\n",
    "\n",
    "# Resources:\n",
    "1. Yt: https://www.youtube.com/watch?v=4Bdc55j80l8\n",
    "2. DataCamp: https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build the encoder layer; specifically: input embeddings and multi-headed attention.\n",
    "\n",
    "<img src=\"assets/attention_layer.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiHeadAttention(nn.Module):\n",
    "    def __init__(self, dims, n_heads):                                  \n",
    "        \"\"\"\n",
    "        dims: Dimensionality of input\n",
    "        n_heads: Number of heads for the attention layer \n",
    "        \"\"\"\n",
    "\n",
    "        super(multiHeadAttention, self).__init__()              # This is fors the torch nn module class\n",
    "        assert dims % n_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        '''\n",
    "        In multi-head attention, the d_model dimension (the overall dimension of each token’s embedding) is split into num_heads \n",
    "        smaller chunks so that each head can process a portion of the model’s dimension independently. The dimension of each head, \n",
    "        called d_k in the code, is calculated as d_model // num_heads. To make this division possible, d_model needs to be evenly \n",
    "        divisible by num_heads.\n",
    "        '''\n",
    "\n",
    "        # Initialize dimensions\n",
    "        self.d_model = dims\n",
    "        self.num_heads = n_heads\n",
    "        self.d_k = dims // n_heads      # Dimension of each head's key, query and value\n",
    "\n",
    "        # Now, time to transform the inputs\n",
    "        self.W_q = nn.Linear(dims, dims)    # Query \n",
    "        self.W_k = nn.Linear(dims, dims)    # Keys \n",
    "        self.W_v = nn.Linear(dims, dims)    # Values\n",
    "        self.W_o = nn.Linear(dims, dims)    # Output\n",
    "\n",
    "\n",
    "    # Now to calculate the attention scores\n",
    "    def attention_dot_product(self, Q, K, V, mask=None):\n",
    "        '''\n",
    "        Q: Query\n",
    "        K: Keys\n",
    "        V: Values\n",
    "        mask: Can be applied to mask out certain attention score values\n",
    "        '''\n",
    "\n",
    "        attn_raw_scores = torch.matmul(Q, K.transpose(-2,-1 ))              # K.transpose(-2, -1) transposes the last two dimensions of the K tensor. (Refer [1].)\n",
    "        scaled_attn_scores = attn_raw_scores/math.sqrt((self.d_k))\n",
    "\n",
    "        # Applying the mask (if not none)\n",
    "        if mask:\n",
    "            scaled_attn_scores = scaled_attn_scores.masked_fill(mask==0, -1e9)      # Refer [2]\n",
    "\n",
    "        # Aplplying softmax activation function to find attention probabilities \n",
    "        attn_probs = torch.softmax(scaled_attn_scores, dim=-1)\n",
    "\n",
    "        # Multiply with the Values to obtain final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially what's going on here is that we obtain the dot product of the queries wrt keys, which is our attention score. It's like yt's search algorithm, where it looks into the database, looks at the values for the keys 'Title', 'Descriptions' and finds matching keys.\n",
    "\n",
    "<img src=\"assets/Calcule_attention_score1.png\">\n",
    "\n",
    "Then, we scale these scores by using d_k which we calculated to split up into multiple heads\n",
    "\n",
    "<img src=\"assets/scaling_attention_scores.png\">\n",
    "\n",
    "[1] What Does K.transpose(-2, -1) Do?\n",
    "K.transpose(-2, -1) transposes the last two dimensions of the K tensor.\n",
    "\n",
    "-2 and -1 in Tensor Indexing: Negative indices count from the end of the tensor shape, so -2 and -1 refer to the second-to-last and last dimensions. \n",
    "In the context of multi-head attention, Q and K typically have the shape:\n",
    "\n",
    "`(batch_size, num_heads, seq_length, d_k)`\n",
    "\n",
    "Here:\n",
    "\n",
    "seq_length is the length of the sequence.\n",
    "d_k is the dimension of each head (i.e., d_model // num_heads).\n",
    "Why Transpose? K.transpose(-2, -1) changes the shape of K to:\n",
    "\n",
    "`(batch_size, num_heads, d_k, seq_length)`\n",
    "This is necessary so that the matrix multiplication torch.matmul(Q, K.transpose(-2, -1)) results in an output shape of (batch_size, num_heads, seq_length, seq_length). This shape represents the attention scores between each position in the sequence (each query) and all other positions (keys), which is essential for calculating the attention distribution across the sequence.\n",
    "\n",
    "[2] \n",
    "\n",
    "`mask == 0`\n",
    "\n",
    "The mask tensor is typically a binary tensor with values of 1 and 0. Here, 1 represents positions we want to keep, and 0 represents positions we want to ignore (mask out).\n",
    "mask == 0 creates a boolean mask where True represents positions that should be masked (ignored), and False represents positions that should be retained.\n",
    "`-1e9`:\n",
    "\n",
    "-1e9 (a very large negative number) is used to \"mask out\" certain positions by setting their attention score to a very low value. When softmax is applied to the attention scores later, this extremely negative value effectively turns the attention probability for masked positions into 0, ensuring they don’t contribute to the weighted sum in the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
