{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Neural Network\n",
    "\n",
    "In this notebood, I will be attempting to create my own transformer neural network from scratch. As many of you know, this is literally how chatGPT and many famous LLMs work under the hood. I'm excited to try this out, to further understand how this stuff ACTUALLY works\n",
    "\n",
    "<img src=\"assets/Screenshot 2024-11-12 144358.png\">\n",
    "\n",
    "Note: I will be using pytorch for this one, but I will later try to implement one from scratch similar to my previous project. But we shall see :salute_face:\n",
    "\n",
    "# Resources:\n",
    "1. Yt: https://www.youtube.com/watch?v=4Bdc55j80l8\n",
    "2. DataCamp: https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build the encoder layer; specifically: input embeddings and multi-headed attention.\n",
    "\n",
    "<img src=\"assets/attention_layer.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiHeadAttention(nn.Module):\n",
    "    def __init__(self, dims_model, n_heads):                                  \n",
    "        \"\"\"\n",
    "        dims_model: Dimensionality of input\n",
    "        n_heads: Number of heads for the attention layer \n",
    "        \"\"\"\n",
    "\n",
    "        super(multiHeadAttention, self).__init__()              # This is fors the torch nn module class\n",
    "        assert dims_model % n_heads == 0, \"dims_model must be divisible by num_heads\"\n",
    "        '''\n",
    "        In multi-head attention, the dims_model dimension (the overall dimension of each token’s embedding) is split into num_heads \n",
    "        smaller chunks so that each head can process a portion of the model’s dimension independently. The dimension of each head, \n",
    "        called d_k in the code, is calculated as dims_model // num_heads. To make this division possible, d_model needs to be evenly \n",
    "        divisible by num_heads.\n",
    "        '''\n",
    "\n",
    "        # Initialize dimensions\n",
    "        self.dims_model = dims_model\n",
    "        self.num_heads = n_heads\n",
    "        self.d_k = dims_model // n_heads      # Dimension of each head's key, query and value\n",
    "\n",
    "        # Now, time to transform the inputs\n",
    "        self.W_q = nn.Linear(dims_model, dims_model)    # Query \n",
    "        self.W_k = nn.Linear(dims_model, dims_model)    # Keys \n",
    "        self.W_v = nn.Linear(dims_model, dims_model)    # Values\n",
    "        self.W_o = nn.Linear(dims_model, dims_model)    # Output\n",
    "\n",
    "\n",
    "    # Now to calculate the attention scores\n",
    "    def attention_dot_product(self, Q, K, V, mask=None):\n",
    "        '''\n",
    "        Q: Query\n",
    "        K: Keys\n",
    "        V: Values\n",
    "        mask: Can be applied to mask out certain attention score values\n",
    "        '''\n",
    "\n",
    "        attn_raw_scores = torch.matmul(Q, K.transpose(-2,-1 ))              # K.transpose(-2, -1) transposes the last two dimensions of the K tensor. (Refer [1].)\n",
    "        scaled_attn_scores = attn_raw_scores/math.sqrt((self.d_k))\n",
    "\n",
    "        # Applying the mask (if not none)\n",
    "        if mask.any():\n",
    "            scaled_attn_scores = scaled_attn_scores.masked_fill(mask==0, -1e9)      # Refer [2]\n",
    "\n",
    "        # Aplplying softmax activation function to find attention probabilities \n",
    "        attn_probs = torch.softmax(scaled_attn_scores, dim=-1)\n",
    "\n",
    "        # Multiply with the Values to obtain final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    # Re-shaping the inputs to have n heads (for multi head attention)\n",
    "    def split_heads(self, x):\n",
    "        # Refer to [1], we are transposing here to get the desired shape of \n",
    "        # (batch_size, num_heads, d_k, seq_length)\n",
    "        batch_size, seq_len, dims_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2)\n",
    "    \n",
    "\n",
    "    # After applying attention to each head separately, we combine the results\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.dims_model)\n",
    "    \n",
    "\n",
    "    # Oh boy, forward propogation time!\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Applying the linear transformations and splitting heads\n",
    "        Q = self.split_heads(self.W_q(Q))       # Basically passing into nn.Linear (linear transformation)\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        # Calculate the dot product (the attention scores)\n",
    "        attn_output = self.attention_dot_product(Q, K, V, mask)\n",
    "\n",
    "        # Combine the outputs from all the heads\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially what's going on here is that we obtain the dot product of the queries wrt keys, which is our attention score. It's like yt's search algorithm, where it looks into the database, looks at the values for the keys 'Title', 'Descriptions' and finds matching keys.\n",
    "\n",
    "<img src=\"assets/Calcule_attention_score1.png\">\n",
    "\n",
    "Then, we scale these scores by using d_k which we calculated to split up into multiple heads\n",
    "\n",
    "<img src=\"assets/scaling_attention_scores.png\">\n",
    "\n",
    "[1] What Does K.transpose(-2, -1) Do?\n",
    "K.transpose(-2, -1) transposes the last two dimensions of the K tensor.\n",
    "\n",
    "-2 and -1 in Tensor Indexing: Negative indices count from the end of the tensor shape, so -2 and -1 refer to the second-to-last and last dimensions. \n",
    "In the context of multi-head attention, Q and K typically have the shape:\n",
    "\n",
    "`(batch_size, num_heads, seq_length, d_k)`\n",
    "\n",
    "Here:\n",
    "\n",
    "seq_length is the length of the sequence.\n",
    "d_k is the dimension of each head (i.e., d_model // num_heads).\n",
    "Why Transpose? K.transpose(-2, -1) changes the shape of K to:\n",
    "\n",
    "`(batch_size, num_heads, d_k, seq_length)`\n",
    "This is necessary so that the matrix multiplication torch.matmul(Q, K.transpose(-2, -1)) results in an output shape of (batch_size, num_heads, seq_length, seq_length). This shape represents the attention scores between each position in the sequence (each query) and all other positions (keys), which is essential for calculating the attention distribution across the sequence.\n",
    "\n",
    "[2] \n",
    "\n",
    "`mask == 0`\n",
    "\n",
    "The mask tensor is typically a binary tensor with values of 1 and 0. Here, 1 represents positions we want to keep, and 0 represents positions we want to ignore (mask out).\n",
    "mask == 0 creates a boolean mask where True represents positions that should be masked (ignored), and False represents positions that should be retained.\n",
    "`-1e9`:\n",
    "\n",
    "-1e9 (a very large negative number) is used to \"mask out\" certain positions by setting their attention score to a very low value. When softmax is applied to the attention scores later, this extremely negative value effectively turns the attention probability for masked positions into 0, ensuring they don’t contribute to the weighted sum in the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the multi-headed attention part, it's time for the Feed Forward part.\n",
    "\n",
    "$$ \\text{FFN}(x) = \\text{Linear}_2(\\text{ReLU}(\\text{Linear}_1(x))) $$\n",
    "\n",
    "This is the general equation for the forward feed. We are basically applying non-linear activation to the outputs of the multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, dims_model, d_ff):\n",
    "        super(positionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(dims_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, dims_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer_one = self.fc1(x)\n",
    "        activation_layer = self.relu(layer_one)\n",
    "        output_layer = self.fc2(activation_layer)\n",
    "\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time for positional encoding. Positional Encoding is used to inject the position information of each token in the input sequence.\n",
    "\n",
    "Even Indices\n",
    "$$\n",
    "PE(p, 2i) = \\sin\\left(\\frac{p}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "Odd Indices\n",
    "$$\n",
    "PE(p, 2i+1) = \\cos\\left(\\frac{p}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(positionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)                     \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)`\n",
    "\n",
    "- *`torch.arange(0, max_seq_length, dtype=torch.float)`*:\n",
    "  - This creates a 1D tensor of floating-point numbers from `0` to `max_seq_length - 1`. For example, if `max_seq_length` is 5, it will create: `[0.0, 1.0, 2.0, 3.0, 4.0]`.\n",
    "  \n",
    "- *`.unsqueeze(1)`*:\n",
    "  - This adds an extra dimension at position `1` (the second dimension). This is important because we want to treat the positions in a sequence as a column vector for each token.\n",
    "  - After this, the shape of `position` will be `(max_seq_length, 1)`. For example, if `max_seq_length` is 5, the result will look like this:\n",
    "    ```\n",
    "    [[0.0],\n",
    "     [1.0],\n",
    "     [2.0],\n",
    "     [3.0],\n",
    "     [4.0]]\n",
    "    ```\n",
    "\n",
    "This tensor represents the position of each token in the sequence (starting from 0, 1, 2, etc.), and we will later use it to calculate the positional encodings for each token in the sequence.\n",
    "\n",
    "2. `div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))`\n",
    "\n",
    "- *`torch.arange(0, d_model, 2)`*:\n",
    "  - This creates a tensor starting from `0` to `d_model - 1`, but it only includes every second number (i.e., it has steps of 2). So, if `d_model = 6`, it creates: `[0, 2, 4]`.\n",
    "\n",
    "- *`.float()`*:\n",
    "  - This converts the tensor into floating-point numbers, so we can perform precise mathematical operations later.\n",
    "\n",
    "- *`-(math.log(10000.0) / d_model)`*:\n",
    "  - This is a scaling factor based on the constant `10000.0`. The logarithm of `10000` is divided by `d_model`. This step is used to scale the frequencies of the sinusoidal functions to get different wavelengths for each dimension in the positional encoding.\n",
    "\n",
    "- *`torch.exp(...)`*:\n",
    "  - The `torch.exp()` function takes the result of the multiplication and applies the exponential function (i.e., raising `e` to the power of the value). This step creates the *divisor terms** for each dimension of the positional encoding, which control how quickly the sine and cosine functions oscillate.\n",
    "\n",
    "Putting It All Together:\n",
    "\n",
    "- *`position`* is a tensor that represents the position of each token in the sequence.\n",
    "- *`div_term`* is a scaling factor (exponentially spaced) that controls the wavelength of each sinusoidal wave used for encoding.\n",
    "  \n",
    "Together, `position` and `div_term` are used to generate the *sinusoidal** positional encoding that is added to the token embeddings, helping the model understand the *relative position* of each token in the sequence.\n",
    "\n",
    "Example in Simple Terms:\n",
    "- The `position` tensor is like a list of \"indices\" (positions 0, 1, 2,... for each token).\n",
    "- The `div_term` tensor defines how the positional encoding will *oscillate** at different frequencies depending on the token's position and the dimension in the encoding.\n",
    "\n",
    "\n",
    "self.dropout: Dropout layer, used to prevent overfitting by randomly setting some activations to zero during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to combine all these pieces to make the encode layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(encoderLayer, self).__init__()\n",
    "        self.attn_layer = multiHeadAttention(d_model, n_heads)\n",
    "        self.feed_forward = positionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.attn_layer(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        feed_forward = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(feed_forward))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Layer Time!\n",
    "<img src='assets/Decoder.png'>\n",
    "\n",
    "x: The input to the decoder layer.\n",
    "\n",
    "enc_output: The output from the corresponding encoder (used in the cross-attention step).\n",
    "\n",
    "src_mask: Source mask to ignore certain parts of the encoder's output.\n",
    "\n",
    "tgt_mask: Target mask to ignore certain parts of the decoder's input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(decoderLayer, self).__init__()\n",
    "        self.self_attn = multiHeadAttention(d_model, n_heads)\n",
    "        self.cross_attn = multiHeadAttention(d_model, n_heads)\n",
    "        self.feed_forward = positionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output =self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Self-Attention with Target Mask (`self_attn`)\n",
    "\n",
    "Purpose\n",
    "In the decoder, self-attention computes relationships **within the target sequence** being generated. For example, when generating the third word, the model should only consider the first two words, not future words. The **target mask (tgt_mask)** ensures that attention weights for future positions are zeroed out, forcing the model to rely only on previously generated tokens.\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "where:\n",
    "- \\( Q \\), \\( K \\), and \\( V \\) are the query, key, and value matrices, respectively.\n",
    "- \\( d_k \\) is the dimensionality of the keys (a scaling factor).\n",
    "\n",
    "After Masking\n",
    "\n",
    "$$\n",
    "\\text{MaskedAttention}(Q, K, V, M) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + M\\right) V\n",
    "$$\n",
    "\n",
    "\n",
    "2. Cross-Attention (`cross_attn`)\n",
    "\n",
    "Purpose\n",
    "Cross-attention allows the decoder to attend to relevant information from the encoder’s output (which represents the processed source sequence). By computing cross-attention, the decoder learns what parts of the input sequence to focus on for generating each token of the output sequence.\n",
    "\n",
    "Mathematical Calculation\n",
    "\n",
    "Cross-attention computes the weighted sum over the encoder output (`V`) based on how similar the keys (`K`) are to the queries (`Q`). The attention formula here is the same as self-attention:\n",
    "$$\n",
    "\\text{CrossAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "After masking:\n",
    "$$\n",
    "\\text{MaskedCrossAttention}(Q, K, V, M_{\\text{src}}) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + M_{\\text{src}}\\right) V\n",
    "$$\n",
    "\n",
    "The **source mask (src_mask)** can be used here to prevent the model from attending to padding tokens in the encoder’s output (e.g., if the source sequence has padding at the end).\n",
    "\n",
    "\n",
    "3. Position-Wise Feed-Forward Network (`feed_forward`)\n",
    "\n",
    "After self-attention and cross-attention, the decoder applies a **position-wise feed-forward network**. This is a simple two-layer network with a ReLU activation that transforms each token representation independently. The formula is:\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "where \\( W_1 \\) and \\( W_2 \\) are learned weights, and \\( b_1 \\) and \\( b_2 \\) are biases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is now time... To put things together and form the\n",
    "# TRANSFORMER!!!\n",
    "\n",
    "<img src=\"assets/Screenshot 2024-11-12 144358.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, n_heads, n_layers, d_ff, max_seq_len, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = positionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        self.encode_layers = [(encoderLayer(d_model, n_heads, d_ff, dropout)) for _ in range(n_layers)]\n",
    "        self.decode_layers = [(decoderLayer(d_model, n_heads, d_ff, dropout)) for _ in range(n_layers)]\n",
    "\n",
    "        self.fully_connected = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask =  (src != 0).unsqueeze(1).unsqueeze(2)        # Gets rid of values that are 0 in the src tensor           Unsqueeze changes the mask shape from [batch_size, seq_length]\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)         # Same for the tgt sensor                                   to [batch_size, 1, 1, seq_length]\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeek_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()                            # refer [1]\n",
    "        tgt_mask = tgt_mask & nopeek_mask\n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedding = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedding = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedding\n",
    "        for enc_layer in self.encode_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedding\n",
    "        for dec_layer in self.decode_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        \n",
    "        output = self.fully_connected(dec_output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]\n",
    "\n",
    "This line creates a **\"no-peek\" mask** for the target sequence in the Transformer. The purpose of the no-peek mask is to ensure that during training or decoding, the model does not look at tokens beyond the current position. This is crucial for auto-regressive tasks like text generation.\n",
    "\n",
    "\n",
    "**`torch.ones(1, seq_length, seq_length)`**\n",
    "- This creates a tensor filled with ones of shape `(1, seq_length, seq_length)`. The shape ensures compatibility with batched processing.\n",
    "- Example when `seq_length = 4`:\n",
    "  ```python\n",
    "  torch.ones(1, 4, 4) ->\n",
    "  [[[1, 1, 1, 1],\n",
    "    [1, 1, 1, 1],\n",
    "    [1, 1, 1, 1],\n",
    "    [1, 1, 1, 1]]]\n",
    "  ```\n",
    "\n",
    "**`torch.triu(..., diagonal=1)`**\n",
    "- `torch.triu` stands for \"upper triangular,\" and it sets all elements **below a certain diagonal** to zero, leaving only the upper triangular part.\n",
    "- The `diagonal=1` argument means the diagonal starts **above the main diagonal**.\n",
    "- Example result of `torch.triu(torch.ones(1, 4, 4), diagonal=1)`:\n",
    "  ```python\n",
    "  [[[0, 1, 1, 1],\n",
    "    [0, 0, 1, 1],\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 0, 0, 0]]]\n",
    "  ```\n",
    "\n",
    "**`1 - ...`**\n",
    "- Subtracting the upper triangular matrix from `1` flips the mask:\n",
    "  - All `1`s in the upper triangular part become `0`s.\n",
    "  - All `0`s in the lower triangular part become `1`s.\n",
    "- Example after `1 - torch.triu(..., diagonal=1)`:\n",
    "  ```python\n",
    "  [[[1, 0, 0, 0],\n",
    "    [1, 1, 0, 0],\n",
    "    [1, 1, 1, 0],\n",
    "    [1, 1, 1, 1]]]\n",
    "  ```\n",
    "\n",
    "**`.bool()`**\n",
    "- Converts the mask from numeric (`0`s and `1`s) to Boolean values (`False` and `True`).\n",
    "- Example result:\n",
    "  ```python\n",
    "  [[[True, False, False, False],\n",
    "    [True, True, False, False],\n",
    "    [True, True, True, False],\n",
    "    [True, True, True, True]]]\n",
    "  ```\n",
    "\n",
    "Why Use This?\n",
    "This mask ensures that when the model is predicting a token at position \\(i\\), it **cannot attend to future tokens \\(j > i\\)**. For example:\n",
    "- When predicting the first token, it can only attend to itself.\n",
    "- When predicting the second token, it can attend to the first and second tokens, and so on.\n",
    "\n",
    "By making the upper triangle 0s, the attention score given to the future tokens are 0. Padding mask is applied for the reverse where we avoid previously generated tokens that are irrelevant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time!\n",
    "\n",
    "Here, I'm going to try to train this model using synthetic data that I generated as well as the Cornell movie dialogs corpus pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = pd.read_csv(r'C:\\Users\\User\\projects\\NeuralNetwork\\data\\tnn_train.csv')\n",
    "inputs = data_csv['Input'].tolist()\n",
    "outputs = data_csv['Output'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "input_tokens = [tokenizer.encode(text, padding='max_length', max_length=32, truncation=True) for text in inputs]\n",
    "output_tokens = [tokenizer.encode(text, padding='max_length', max_length=32, truncation=True) for text in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor(input_tokens)\n",
    "output_tensor = torch.tensor(output_tokens)\n",
    "\n",
    "dataset = data.TensorDataset(input_tensor, output_tensor)\n",
    "dataloader = data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransformerModel = Transformer(\n",
    "    src_vocab_size=30000,\n",
    "    tgt_vocab_size=30000,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    n_layers=6,\n",
    "    d_ff=2048,\n",
    "    max_seq_len=30000,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)  \n",
    "optimizer = optim.Adam(TransformerModel.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatModel(nn.Module):\n",
    "    def __init__(self, transformer, tokenizer, d_model):\n",
    "        super(ChatModel, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.linear = nn.Linear(d_model, len(tokenizer))  # Output vocab size\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Pass source and target through the transformer\n",
    "        output = self.transformer(src, tgt)\n",
    "        return self.linear(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Reshape output for loss calculation\u001b[39;00m\n\u001b[0;32m     20\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 21\u001b[0m tgt_output \u001b[38;5;241m=\u001b[39m \u001b[43mtgt_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Compute loss and backpropagate\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, tgt_output)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# Training main loop\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    TransformerModel.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        src, tgt = batch\n",
    "\n",
    "        # Shift target tokens for decoder input\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = TransformerModel(src, tgt_input)\n",
    "\n",
    "        # Reshape output for loss calculation\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        tgt_output = tgt_output.view(-1)\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = criterion(output, tgt_output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {epoch_loss:.4f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(model, tokenizer, input_text):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    response_ids = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\n",
    "    return tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "\n",
    "input_text = \"How are you?\"\n",
    "response = chat(model, tokenizer, input_text)\n",
    "print(\"Bot:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
