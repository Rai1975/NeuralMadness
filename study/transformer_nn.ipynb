{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Neural Network\n",
    "\n",
    "In this notebood, I will be attempting to create my own transformer neural network from scratch. As many of you know, this is literally how chatGPT and many famous LLMs work under the hood. I'm excited to try this out, to further understand how this stuff ACTUALLY works\n",
    "\n",
    "<img src=\"assets/Screenshot 2024-11-12 144358.png\">\n",
    "\n",
    "Note: I will be using pytorch for this one, but I will later try to implement one from scratch similar to my previous project. But we shall see :salute_face:\n",
    "\n",
    "# Resources:\n",
    "1. Yt: https://www.youtube.com/watch?v=4Bdc55j80l8\n",
    "2. DataCamp: https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build the encoder layer; specifically: input embeddings and multi-headed attention.\n",
    "\n",
    "<img src=\"assets/attention_layer.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiHeadAttention(nn.Module):\n",
    "    def __init__(self, dims_model, n_heads):                                  \n",
    "        \"\"\"\n",
    "        dims_model: Dimensionality of input\n",
    "        n_heads: Number of heads for the attention layer \n",
    "        \"\"\"\n",
    "\n",
    "        super(multiHeadAttention, self).__init__()              # This is fors the torch nn module class\n",
    "        assert dims_model % n_heads == 0, \"dims_model must be divisible by num_heads\"\n",
    "        '''\n",
    "        In multi-head attention, the dims_model dimension (the overall dimension of each token’s embedding) is split into num_heads \n",
    "        smaller chunks so that each head can process a portion of the model’s dimension independently. The dimension of each head, \n",
    "        called d_k in the code, is calculated as dims_model // num_heads. To make this division possible, d_model needs to be evenly \n",
    "        divisible by num_heads.\n",
    "        '''\n",
    "\n",
    "        # Initialize dimensions\n",
    "        self.dims_model = dims_model\n",
    "        self.num_heads = n_heads\n",
    "        self.d_k = dims_model // n_heads      # Dimension of each head's key, query and value\n",
    "\n",
    "        # Now, time to transform the inputs\n",
    "        self.W_q = nn.Linear(dims_model, dims_model)    # Query \n",
    "        self.W_k = nn.Linear(dims_model, dims_model)    # Keys \n",
    "        self.W_v = nn.Linear(dims_model, dims_model)    # Values\n",
    "        self.W_o = nn.Linear(dims_model, dims_model)    # Output\n",
    "\n",
    "\n",
    "    # Now to calculate the attention scores\n",
    "    def attention_dot_product(self, Q, K, V, mask=None):\n",
    "        '''\n",
    "        Q: Query\n",
    "        K: Keys\n",
    "        V: Values\n",
    "        mask: Can be applied to mask out certain attention score values\n",
    "        '''\n",
    "\n",
    "        attn_raw_scores = torch.matmul(Q, K.transpose(-2,-1 ))              # K.transpose(-2, -1) transposes the last two dimensions of the K tensor. (Refer [1].)\n",
    "        scaled_attn_scores = attn_raw_scores/math.sqrt((self.d_k))\n",
    "\n",
    "        # Applying the mask (if not none)\n",
    "        if mask:\n",
    "            scaled_attn_scores = scaled_attn_scores.masked_fill(mask==0, -1e9)      # Refer [2]\n",
    "\n",
    "        # Aplplying softmax activation function to find attention probabilities \n",
    "        attn_probs = torch.softmax(scaled_attn_scores, dim=-1)\n",
    "\n",
    "        # Multiply with the Values to obtain final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    # Re-shaping the inputs to have n heads (for multi head attention)\n",
    "    def split_heads(self, x):\n",
    "        # Refer to [1], we are transposing here to get the desired shape of \n",
    "        # (batch_size, num_heads, d_k, seq_length)\n",
    "        batch_size, seq_len, dims_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2)\n",
    "    \n",
    "\n",
    "    # After applying attention to each head separately, we combine the results\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.dims_model)\n",
    "    \n",
    "\n",
    "    # Oh boy, forward propogation time!\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Applying the linear transformations and splitting heads\n",
    "        Q = self.split_heads(self.W_q(Q))       # Basically passing into nn.Linear (linear transformation)\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        # Calculate the dot product (the attention scores)\n",
    "        attn_output = self.attention_dot_product(Q, K, V, mask)\n",
    "\n",
    "        # Combine the outputs from all the heads\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially what's going on here is that we obtain the dot product of the queries wrt keys, which is our attention score. It's like yt's search algorithm, where it looks into the database, looks at the values for the keys 'Title', 'Descriptions' and finds matching keys.\n",
    "\n",
    "<img src=\"assets/Calcule_attention_score1.png\">\n",
    "\n",
    "Then, we scale these scores by using d_k which we calculated to split up into multiple heads\n",
    "\n",
    "<img src=\"assets/scaling_attention_scores.png\">\n",
    "\n",
    "[1] What Does K.transpose(-2, -1) Do?\n",
    "K.transpose(-2, -1) transposes the last two dimensions of the K tensor.\n",
    "\n",
    "-2 and -1 in Tensor Indexing: Negative indices count from the end of the tensor shape, so -2 and -1 refer to the second-to-last and last dimensions. \n",
    "In the context of multi-head attention, Q and K typically have the shape:\n",
    "\n",
    "`(batch_size, num_heads, seq_length, d_k)`\n",
    "\n",
    "Here:\n",
    "\n",
    "seq_length is the length of the sequence.\n",
    "d_k is the dimension of each head (i.e., d_model // num_heads).\n",
    "Why Transpose? K.transpose(-2, -1) changes the shape of K to:\n",
    "\n",
    "`(batch_size, num_heads, d_k, seq_length)`\n",
    "This is necessary so that the matrix multiplication torch.matmul(Q, K.transpose(-2, -1)) results in an output shape of (batch_size, num_heads, seq_length, seq_length). This shape represents the attention scores between each position in the sequence (each query) and all other positions (keys), which is essential for calculating the attention distribution across the sequence.\n",
    "\n",
    "[2] \n",
    "\n",
    "`mask == 0`\n",
    "\n",
    "The mask tensor is typically a binary tensor with values of 1 and 0. Here, 1 represents positions we want to keep, and 0 represents positions we want to ignore (mask out).\n",
    "mask == 0 creates a boolean mask where True represents positions that should be masked (ignored), and False represents positions that should be retained.\n",
    "`-1e9`:\n",
    "\n",
    "-1e9 (a very large negative number) is used to \"mask out\" certain positions by setting their attention score to a very low value. When softmax is applied to the attention scores later, this extremely negative value effectively turns the attention probability for masked positions into 0, ensuring they don’t contribute to the weighted sum in the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the multi-headed attention part, it's time for the Feed Forward part.\n",
    "\n",
    "$$ \\text{FFN}(x) = \\text{Linear}_2(\\text{ReLU}(\\text{Linear}_1(x))) $$\n",
    "\n",
    "This is the general equation for the forward feed. We are basically applying non-linear activation to the outputs of the multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, dims_model, d_ff):\n",
    "        super(positionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(dims_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, dims_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer_one = self.fc1(x)\n",
    "        activation_layer = self.relu(layer_one)\n",
    "        output_layer = self.fc2(activation_layer)\n",
    "\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time for positional encoding. Positional Encoding is used to inject the position information of each token in the input sequence.\n",
    "\n",
    "Even Indices\n",
    "$$\n",
    "PE(p, 2i) = \\sin\\left(\\frac{p}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "Odd Indices\n",
    "$$\n",
    "PE(p, 2i+1) = \\cos\\left(\\frac{p}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(positionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)                     \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)`\n",
    "\n",
    "- *`torch.arange(0, max_seq_length, dtype=torch.float)`*:\n",
    "  - This creates a 1D tensor of floating-point numbers from `0` to `max_seq_length - 1`. For example, if `max_seq_length` is 5, it will create: `[0.0, 1.0, 2.0, 3.0, 4.0]`.\n",
    "  \n",
    "- *`.unsqueeze(1)`*:\n",
    "  - This adds an extra dimension at position `1` (the second dimension). This is important because we want to treat the positions in a sequence as a column vector for each token.\n",
    "  - After this, the shape of `position` will be `(max_seq_length, 1)`. For example, if `max_seq_length` is 5, the result will look like this:\n",
    "    ```\n",
    "    [[0.0],\n",
    "     [1.0],\n",
    "     [2.0],\n",
    "     [3.0],\n",
    "     [4.0]]\n",
    "    ```\n",
    "\n",
    "This tensor represents the position of each token in the sequence (starting from 0, 1, 2, etc.), and we will later use it to calculate the positional encodings for each token in the sequence.\n",
    "\n",
    "2. `div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))`\n",
    "\n",
    "- *`torch.arange(0, d_model, 2)`*:\n",
    "  - This creates a tensor starting from `0` to `d_model - 1`, but it only includes every second number (i.e., it has steps of 2). So, if `d_model = 6`, it creates: `[0, 2, 4]`.\n",
    "\n",
    "- *`.float()`*:\n",
    "  - This converts the tensor into floating-point numbers, so we can perform precise mathematical operations later.\n",
    "\n",
    "- *`-(math.log(10000.0) / d_model)`*:\n",
    "  - This is a scaling factor based on the constant `10000.0`. The logarithm of `10000` is divided by `d_model`. This step is used to scale the frequencies of the sinusoidal functions to get different wavelengths for each dimension in the positional encoding.\n",
    "\n",
    "- *`torch.exp(...)`*:\n",
    "  - The `torch.exp()` function takes the result of the multiplication and applies the exponential function (i.e., raising `e` to the power of the value). This step creates the *divisor terms** for each dimension of the positional encoding, which control how quickly the sine and cosine functions oscillate.\n",
    "\n",
    "Putting It All Together:\n",
    "\n",
    "- *`position`* is a tensor that represents the position of each token in the sequence.\n",
    "- *`div_term`* is a scaling factor (exponentially spaced) that controls the wavelength of each sinusoidal wave used for encoding.\n",
    "  \n",
    "Together, `position` and `div_term` are used to generate the *sinusoidal** positional encoding that is added to the token embeddings, helping the model understand the *relative position* of each token in the sequence.\n",
    "\n",
    "Example in Simple Terms:\n",
    "- The `position` tensor is like a list of \"indices\" (positions 0, 1, 2,... for each token).\n",
    "- The `div_term` tensor defines how the positional encoding will *oscillate** at different frequencies depending on the token's position and the dimension in the encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to combine all these pieces to make the encode layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(encoderLayer, self).__init__()\n",
    "        self.attn_layer = multiHeadAttention(d_model, n_heads)\n",
    "        self.feed_forward = positionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def foward(self, x, mask):\n",
    "        attn_output = self.attn_layer(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        feed_forward = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(feed_forward))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
