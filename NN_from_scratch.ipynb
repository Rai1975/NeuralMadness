{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I'm going to attempt to build a neural network from scratch, without using pytorch or tensorflow. This project is healily inspired by these two videos:\n",
    "1. https://www.youtube.com/watch?v=w8yWXqWQYmU\n",
    "2. https://www.youtube.com/watch?v=cAkMcPfY_Ns\n",
    "\n",
    "This is going to be a very simple model. I'm using the MNIST database to train this network on handwritten variations of numbers, and identifying them correctly (hopefully)\n",
    "This is going to be a simple network with basically 3 layers, an input layer and 2 hidden layers leading to an output. The first hidden layer will be a ReLU function and the second layer will implement a simple a softmax activation function (similar to the first link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "test_data = pd.read_csv('data/test.csv') \n",
    "train_data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting into numpy array\n",
    "train_data_numpy = np.array(train_data)\n",
    "m, n = train_data_numpy.shape\n",
    "\n",
    "data_train = train_data_numpy.T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 7, 6, 9], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now this is the real stuff, moving onto the fundamentals of the network\n",
    "\n",
    "# Parameters for the back propogation\n",
    "def init_params():\n",
    "    W1 = np.random.randn(10, 784) - 0.5  # (We are using 784/784 resolution pictrues)\n",
    "    b1 = np.random.randn(10, 1) - 0.5    # This is for the hidden layer, where we determine if it's a number between 1 and 10.\n",
    "    W2 = np.random.randn(10,10) - 0.5    # Similar, but for the second hidden layer\n",
    "    b2 = np.random.randn(10,1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Rectified Linear Unit\n",
    "def ReLU(x):\n",
    "    for i in len(x):\n",
    "        x[i] = x[i] if x[i] > 0 else 0\n",
    "\n",
    "# Softmax activation function\n",
    "def softmax(x):\n",
    "    sum_exp = sum(np.exp(x))\n",
    "    softmax = np.exp(x)/sum_exp\n",
    "\n",
    "    return softmax\n",
    "\n",
    "def encode_answers(x):\n",
    "    enc = np.zeros((x.size, x.max() + 1))\n",
    "    enc[np.arrange(x.size), x] = 1\n",
    "    enc = enc.T\n",
    "\n",
    "    return enc\n",
    "\n",
    "def forward(W1, b1, W2, b2, x):\n",
    "    Z1 = W1.dot(x) + b1\n",
    "    A1 = ReLU(Z1)           # First hidden layer\n",
    "    Z2 = W2.dot(x) + b2\n",
    "    A2 = softmax(Z2)        # Second hidden layer\n",
    "\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def backward(Z1, A1, Z2, A2, W1, W2, x, Y):\n",
    "    encoding = encode_answers(Y)\n",
    "    dZ2 = A2 - encoding\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * (Z1 > 0)\n",
    "    dW1 = 1 / m * dZ1.dot(x.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    " \n",
    "def update_params(W1, b1, W2, b2, dW1, dW2, db1, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
